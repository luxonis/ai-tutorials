{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.luxonis.com/logo.svg\" width=\"400\">\n",
    "\n",
    "# Training an Instance Segmentation Model from a Roboflow Dataset\n",
    "\n",
    "## üåü Overview\n",
    "In this tutorial, we'll go through the process of training a custom instance segmentation model using a dataset from Roboflow Universe. We'll find a suitable dataset and import it to LuxonisTrain pipeline. We'll also validate the performance of our model, export it, and make it ready for deployment on a Luxonis device.\n",
    "\n",
    "## üìú Table of Contents\n",
    "- [üõ†Ô∏è Installation](#Ô∏èinstallation)\n",
    "- [üóÉÔ∏è Data Preparation](#data-preparation)\n",
    "- [üèãÔ∏è‚Äç‚ôÇÔ∏è Training](#Ô∏èÔ∏ètraining)\n",
    "    - [‚öôÔ∏è Configuration](#Ô∏èconfiguration)\n",
    "    - [ü¶æ Train](#train)\n",
    "- [‚úç Test](#test)\n",
    "    - [üß† Infer](#infer)\n",
    "- [üóÇÔ∏è Export and Archive](#export-and-archive)\n",
    "- [ü§ñ Deploy](#deploy)\n",
    "- [üì∑ DepthAI Script](#depthai-script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Ô∏èinstallation\"></a>\n",
    "\n",
    "## üõ†Ô∏è Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The main focus of this tutorial is using [`LuxonisTrain`](https://github.com/luxonis/luxonis-train), a user-friendly tool designed to streamline the training of deep learning models, especially for edge devices. We'll also use [`LuxonisML`](https://github.com/luxonis/luxonis-ml) since it provides us with a collection of utility functionality. We will download the dataset from a popular open-source annotated data repostiory - [Roboflow](https://roboflow.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q datasets>=3.1.0 luxonis-train==0.3.8 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"data-preparation\"></a>\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Find a dataset that best matches your use case on [Roboflow Universe](https://universe.roboflow.com/). For this demo we chose [PCB defects dataset](https://universe.roboflow.com/learn-uzoux/pcb-defect-0i1a7) that has annotated soldering quality for SMD components.\n",
    "\n",
    "To get the dataset in for training go to `Dataset tab` in the Roboflow project page, select the desired version and copy the URL. From this URL you will need to extract the data in order for the final line to look like: \n",
    "\n",
    "    roboflow://<TEAM_NAME>/<DATASET_NAME>/<DATASET_VERSION>/coco\n",
    "\n",
    "Our URL was: https://universe.roboflow.com/learn-uzoux/pcb-defect-0i1a7 and the end line looks like:\n",
    "\n",
    "    roboflow://learn-uzoux/pcb-defect-0i1a7/2/coco\n",
    "\n",
    "You will need this in the following step. \n",
    "\n",
    "üí° Tip (Optional): If the original dataset is not good enough for your usecase, you can expand it by uploading your own custom images and annotations using the Roboflow web interface. Follow Roboflow's [official tutorial](https://blog.roboflow.com/getting-started-with-roboflow/) here to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"Ô∏èÔ∏ètraining\"></a>\n",
    "\n",
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"Ô∏èconfiguration\"></a>\n",
    "\n",
    "### ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared the dataset and are almost ready for the actual training. The last step is just to set up our training configuration file. The whole training process in `LuxonisTrain` doesn't require any coding. We advise you to take one of the base configuration files from [here](https://github.com/luxonis/luxonis-train/tree/main/configs) depending on the task, and then edit it to fit your needs.\n",
    "\n",
    "In our case, since we are training an instance segmentation model, we'll take a [`instance_segmentation_light_model.yaml`](https://github.com/luxonis/luxonis-train/blob/main/configs/instance_segmentation_light_model.yaml) config as a starting point, which downloads pre-trained COCO weights, making it ideal for fine-tuning. There are many parameters that we can change, and we advise you to go through the [`documentation`](https://github.com/luxonis/luxonis-train/blob/main/configs/README.md) to find all of them. In this tutorial, we'll only go through some basic ones to get you started on your journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a starting point for our config. As mentioned before, we already made some changes to it, so it works with this tutorial (model name and dataset name change), but feel free to edit it further and make it your own. When you are done editing, you can execute the cell, and the file will be written and ready to use.\n",
    "\n",
    "Make sure you point the training of the model to your own dataset by changing the parameter `dataset_dir`.\n",
    "\n",
    "**Note**: In case you don't have enough computing power on your machine, you can either use [Google Colab](https://colab.research.google.com/) (with GPU enabled), or you can try tweaking the training hyperparameters (such as lowering number of epochs or batch size). However, please be aware that bad parametrization can result in worse performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile solder_defect_segmentation_model_config.yaml\n",
    "model:\n",
    "  name: pcb_defects_instance_seg\n",
    "  predefined_model:\n",
    "    name: InstanceSegmentationModel\n",
    "    params:\n",
    "      variant: heavy\n",
    "      loss_params:\n",
    "        bbox_loss_weight: 60 # Should be 7.5 * accumulate_grad_batches for best results\n",
    "        class_loss_weight: 4 # Should be 0.5 * accumulate_grad_batches for best results\n",
    "        dfl_loss_weight: 12 # Should be 1.5 * accumulate_grad_batches for best results\n",
    "\n",
    "loader:\n",
    "  params:\n",
    "    dataset_dir: \"roboflow://learn-uzoux/pcb-defect-0i1a7/2/coco\" # Change this link to your dataset\n",
    "\n",
    "rich_logging: false\n",
    "\n",
    "trainer:\n",
    "  precision: \"16-mixed\"\n",
    "  preprocessing:\n",
    "    train_image_size: [384, 512]\n",
    "    keep_aspect_ratio: true\n",
    "    normalize:\n",
    "      active: true\n",
    "      params:\n",
    "        mean: [0., 0., 0.]\n",
    "        std: [1, 1, 1]\n",
    "    augmentations:\n",
    "    - name: Rotate\n",
    "      params:\n",
    "        limit: 3\n",
    "        p: 0.5\n",
    "        border_mode: 0\n",
    "        value: [0, 0, 0]\n",
    "    - name: Perspective\n",
    "      params:\n",
    "        scale: [0.02, 0.04]\n",
    "        keep_size: true\n",
    "        pad_mode: 0\n",
    "        pad_val: 0\n",
    "        mask_pad_val: 0\n",
    "        fit_output: false\n",
    "        interpolation: 1\n",
    "        always_apply: false\n",
    "        p: 0.5\n",
    "    - name: Affine\n",
    "      params:\n",
    "        scale: 1\n",
    "        translate_percent: 0\n",
    "        translate_px: null\n",
    "        shear: 5\n",
    "        interpolation: 1\n",
    "        mask_interpolation: 0\n",
    "        cval: 0\n",
    "        cval_mask: 0\n",
    "        mode: 0\n",
    "        fit_output: false\n",
    "        keep_ratio: false\n",
    "        rotate_method: largest_box\n",
    "        always_apply: false\n",
    "        p: 0.4\n",
    "    - name: MotionBlur\n",
    "      params:\n",
    "        blur_limit: [2,4]\n",
    "        p: 0.1\n",
    "    - name: ToGray\n",
    "      params:\n",
    "        p: 0.5\n",
    "    - name: RandomBrightnessContrast\n",
    "      params:\n",
    "        ensure_safe_range: True\n",
    "        p: 0.3\n",
    "    - name: GaussNoise\n",
    "      params:\n",
    "        std_range: [0.0, 0.05]\n",
    "        p: 0.1\n",
    "\n",
    "  batch_size: 8\n",
    "  epochs: &epochs 200\n",
    "  n_workers: 4\n",
    "  validation_interval: 10\n",
    "  n_log_images: 8\n",
    "  gradient_clip_val: 10\n",
    "\n",
    "  callbacks:\n",
    "    - name: EMACallback\n",
    "      params:\n",
    "        decay: 0.9999\n",
    "        use_dynamic_decay: True\n",
    "        decay_tau: 2000\n",
    "    - name: ExportOnTrainEnd\n",
    "    - name: TestOnTrainEnd\n",
    "    - name: GradientAccumulationScheduler\n",
    "      params:\n",
    "        scheduling: # warmup phase is 3 epochs\n",
    "          0: 1\n",
    "          1: 4\n",
    "          2: 8 # For best results, always accumulate gradients to effectively use 64 batch size\n",
    "\n",
    "  training_strategy:\n",
    "    name: \"TripleLRSGDStrategy\"\n",
    "    params:\n",
    "      warmup_epochs: 3\n",
    "      warmup_bias_lr: 0.0\n",
    "      warmup_momentum: 0.8\n",
    "      lr: 0.01\n",
    "      lre: 0.0001\n",
    "      momentum: 0.937\n",
    "      weight_decay: 0.0005\n",
    "      nesterov: True\n",
    "      cosine_annealing: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"train\"></a>\n",
    "\n",
    "### ü¶æ Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the training, we need to initialize the `LuxonisModel`, pass it the path to the configuration file, and call the `train()` method on it. If you are training the model on your private Roboflow dataset you will have to enter also your Roboflow API KEY.\n",
    "\n",
    "**Note**: LuxonisTrain also supports all these commands through usage of its CLI ([docs here](https://github.com/luxonis/luxonis-train/tree/main?tab=readme-ov-file#-cli)), no code required. We won't use them for tutorial purposes, but feel free to use them when you do it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luxonis_train import LuxonisModel\n",
    "import os\n",
    "\n",
    "# If you are using your private dataset from Roboflow, you must enter your API key\n",
    "os.environ['ROBOFLOW_API_KEY'] = '<ADD_YOUR_ROBOFLOW_API_KEY_HERE>'\n",
    "config_path = \"solder_defect_segmentation_model_config.yaml\"\n",
    "\n",
    "luxonis_model = LuxonisModel(config_path)\n",
    "luxonis_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LuxonisTrain` has also already implemented automatic tracking of training runs. By default, `Tensorboard` is used, and to look at the losses, metrics, and visualizations during training, we can inspect the logs. If you check the `output` folder, you'll see that every run creates a new directory, and each run also has its training logs in the `./output/tensorboard_logs` where the name of the folder matches the run's name. To make all the subsequent commands work automatically, please set the name of your run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = \"<YOUR_RUN_NAME>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output/tensorboard_logs # TODO: Change the name of the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir output/tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"test\"></a>\n",
    "\n",
    "## ‚úç Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a trained model that performs well on the validation set. The next step is to check its performance on the testing set, a collection of images we've kept hidden from the model. It should only be used to evaluate whether the model is good objectively. Since this is an instance segmentation task, we use the Mean Average Precision, Mean Average Recall, and F1 Score metrics to check the model performance quantitatively.\n",
    "\n",
    "If you check out the run directory, you'll see two folders inside: `best_val_metric` and `min_val_loss`. Both have checkpoint files generated during training based on best validation metric performance and minimal validation loss. For evaluation, we'll want to use one of these checkpoints; we recommend that you use one that has the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = luxonis_model.get_min_loss_checkpoint_path() # gets checkpoint where validation loss was the lowest\n",
    "# weights = luxonis_model.get_best_metric_checkpoint_path() # gets checkpoint where validation metric was the highest\n",
    "\n",
    "metrics = luxonis_model.test(view=\"test\", weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"infer\"></a>\n",
    "\n",
    "### üß† Infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we also want to visualize the prediction of the trained model on test images to ensure it does what it is supposed to do. This is called inference, and we can perform it either on one of the views (e.g., test) or a random image, directory of images, or whole video (for more details, refer to the [docs](https://github.com/luxonis/luxonis-train/tree/main?tab=readme-ov-file#inference)). In our case, we'll infer the model on test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you are using Google Colab use this and images will be saved to \"infer_results_beans_model\" directory\n",
    "luxonis_model.infer(weights=weights, save_dir=\"infer_results_defects_model2\", view=\"test\")\n",
    "\n",
    "# NOTE: If you are not using Google Colab use this and images will be displayed\n",
    "# luxonis_model.infer(\n",
    "#     weights=weights,\n",
    "#     view=\"test\"\n",
    "# ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize one of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='infer_results_defects_model2/-PrecisionSegmentBBoxHead_InstanceSegmentationVisualizer_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"export-and-archive\"></a>\n",
    "\n",
    "## üóÇÔ∏è Export and Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained and tested, we want to prepare it for deployment on the device. This preparation consists of 2 steps. First, we want to export the model trained with PyTorch to a more general format called [`Open Neural Network Exchange (ONNX)`](https://onnx.ai/). Then, we want to package this exported model with all the metadata containing information about the inputs, outputs, and training configuration used. This is called archiving. These steps can be done quickly with just one command in `LuxonisTrain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archieve_path = luxonis_model.archive(weights=weights)\n",
    "print(\"Model archieved to:\", archieve_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that two new folders were created in our run directory. One is called `export` and has an ONNX model while the other is called `archive` which has `.tar.xz` file. The tar file is a compressed file that holds the aforementioned ONNX model with all the model metadata.\n",
    "\n",
    "To use the NN archive for conversion, please copy the path to the .tar.xz file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_ARCHIVE_PATH = \"output/6-emerald-wildfowl/archive/pcb_defects_instance_seg.onnx.tar.xz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"deploy\"></a>\n",
    "\n",
    "## ü§ñ Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully exported and archived the model, we aim to deploy it to the Luxonis device. The model's specific format depends on the Luxonis device series you have. We will show you how to use our [`ModelConverter`](https://github.com/luxonis/modelconverter) to convert the model as simply as possible. \n",
    "\n",
    "We'll start by installing the `ModelConverter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q modelconv==0.4.0 -U\n",
    "%pip  install -U --no-deps \"modelconv @ git+https://github.com/luxonis/modelconverter.git@fix/keyring-timeout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `ModelConverter` Python API, which leverages our [`HubAI`](https://hub.luxonis.com) platform to perform model conversion in the background. To get started, you'll need to create an account on `HubAI` and obtain your team‚Äôs API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUBAI_API_KEY = \"<YOUR_HUBAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model conversion can be done via either the CLI or the Python API ‚Äî here, we'll use the latter. For more information, see the [online usage section](https://github.com/luxonis/modelconverter?tab=readme-ov-file#online-usage) of the documentation.\n",
    "\n",
    "The call below creates a new model card within your team on `HubAI`, uploads the model file and metadata, then performs cloud-side conversion to the selected target platform (e.g., [`RVC2`](https://rvc4.docs.luxonis.com/hardware/platform/rvc/rvc2/), [`RVC4`](https://rvc4.docs.luxonis.com/hardware/platform/rvc/rvc4/)). Once completed, the converted model is automatically downloaded to your device.\n",
    "\n",
    "For HubAI-specific conversion parameters, refer to the [online conversion section](https://github.com/luxonis/modelconverter/tree/e6a3478ba47d8f92d4d60217f2aee0f4f468cb14/modelconverter/hub#online-conversion) of the ModelConverter documentation. Platform-specific parameters are also documented there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelconverter import convert\n",
    "\n",
    "converted_model = convert.RVC4(\n",
    "    api_key=HUBAI_API_KEY,\n",
    "    path=NN_ARCHIVE_PATH,\n",
    "    name=\"Solder Defect Instance Segmentation1\",\n",
    "    description_short=\"Solder Defect Instance Segmentation Model\",\n",
    "    tasks=[\"INSTANCE_SEGMENTATION\"],\n",
    "    license_type=\"MIT\",\n",
    "    is_public=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully converted our trained model for an RVC4 device, so let's test it! Please copy the path to the downloaded archive with the converted model from the output log of the last code cell; we will use it in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"<YOUR_DOWNLOADED_MODEL_ARCHIVE_PATH>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name =\"depthai-script\"></a>\n",
    "\n",
    "## üì∑ DepthAI Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our model on one of our cameras, we first need to install [`DepthAI`](https://rvc4.docs.luxonis.com/software/) in version 3 and [`DepthAI Nodes`](https://rvc4.docs.luxonis.com/software/ai-inference/depthai-nodes/). Moreover, the script we'll write must run locally and require a Luxonis device connected to your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q depthai==3.0.0rc2 -U\n",
    "%pip install -q depthai-nodes==0.3.0 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the model on a DepthAI device using the script below, please note the following:\n",
    "\n",
    "- You can view the output stream by opening [http://localhost:8082](http://localhost:8082) in your browser.\n",
    "\n",
    "- If you're running the script from a Jupyter Notebook, the output may not appear directly within the notebook. The script should print a link pointing to [http://localhost:8082](http://localhost:8082) for accessing the stream.\n",
    "\n",
    "- To stop the video stream, press **`q`** while focused on the visualizer page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = None # Set to None to use the default device, or you can specify a specific device IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import depthai as dai\n",
    "from depthai_nodes.node import ParsingNeuralNetwork, ApplyColormap, ImgFrameOverlay\n",
    "\n",
    "device = dai.Device(dai.DeviceInfo(DEVICE)) if DEVICE else dai.Device()\n",
    "platform = device.getPlatform()\n",
    "img_frame_type = dai.ImgFrame.Type.BGR888i if platform.name == \"RVC4\" else dai.ImgFrame.Type.BGR888p\n",
    "visualizer = dai.RemoteConnection(httpPort=8082)\n",
    "\n",
    "with dai.Pipeline(device) as pipeline:\n",
    "    cam = pipeline.create(dai.node.Camera).build()\n",
    "    nn_archive = dai.NNArchive(MODEL_PATH)\n",
    "    # Create the neural network node\n",
    "    nn_with_parser = pipeline.create(ParsingNeuralNetwork).build(\n",
    "        cam.requestOutput((640, 640), type=img_frame_type, fps=30), \n",
    "        nn_archive\n",
    "    )\n",
    "    # transform output array to colormap\n",
    "    apply_colormap_node = pipeline.create(ApplyColormap).build(nn_with_parser.out)\n",
    "    # overlay frames\n",
    "    overlay_frames_node = pipeline.create(ImgFrameOverlay).build(\n",
    "        nn_with_parser.passthrough,\n",
    "        apply_colormap_node.out,\n",
    "    ) \n",
    "    # Configure the visualizer node\n",
    "    visualizer.addTopic(\"Video\", overlay_frames_node.out, \"images\")\n",
    "    visualizer.addTopic(\"Detections\", nn_with_parser.out, \"detections\")\n",
    "\n",
    "    pipeline.start()\n",
    "    visualizer.registerPipeline(pipeline)\n",
    "\n",
    "    while pipeline.isRunning():\n",
    "        key = visualizer.waitKey(1)\n",
    "        if key == ord(\"q\"):\n",
    "            print(\"Got q key from the remote connection!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! üéâüéâüéâ Huge congratulations, you have successfully finished this tutorial in which you deployed an instance segmentation model trained using `luxonis-train` on a Cracks and Spallings dataset to our camera!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
