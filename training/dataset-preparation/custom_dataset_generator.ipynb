{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9de6101",
   "metadata": {},
   "source": [
    "<img src=\"https://www.luxonis.com/logo.svg\" width=\"400\">\n",
    "\n",
    "# üì¶ Creating an LDF Dataset Using a Custom Generator\n",
    "\n",
    "## üåü Overview\n",
    "In this tutorial, we'll walk through the process of creating a Luxonis Dataset Format (LDF) that can be used to train AI models using custom data generators.\n",
    "\n",
    "---\n",
    "\n",
    "## üìú Table of Contents\n",
    "\n",
    "- [üõ†Ô∏è Installation](#installation)\n",
    "- [üì• Download COCO People Subset Dataset](#download-coco-people-subset-dataset)\n",
    "- [‚öôÔ∏è Creating a Custom Generator](#creating-a-custom-generator)\n",
    "- [üèãÔ∏è‚Äç‚ôÇÔ∏è Creating the Dataset and Making Splits](#creating-the-dataset-and-making-splits)\n",
    "- [üìä Inspecting the Dataset via Loader](#inspecting-the-dataset-via-loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765abf00",
   "metadata": {},
   "source": [
    "<a name=\"Ô∏èinstallation\"></a>\n",
    "\n",
    "## üõ†Ô∏è Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c28cd",
   "metadata": {},
   "source": [
    "The primary goal of this tutorial is to demonstrate how to use[`LuxonisML`](https://github.com/luxonis/luxonis-ml)  for creating  computer vision datasets in the Luxonis Data Format (LDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8efee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q luxonis-ml[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import cv2\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from luxonis_ml.data import LuxonisDataset, LuxonisLoader, DatasetIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c2791",
   "metadata": {},
   "source": [
    "<a name=\"download-coco-people-subset-dataset\"></a>\n",
    "\n",
    "## üì• Download COCO People Subset Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9ddf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://drive.google.com/uc?id=1XlvFK7aRmt8op6-hHkWVKIJQeDtOwoRT\"\n",
    "output_zip = \"../data/COCO_people_subset.zip\"\n",
    "output_folder = \"../data/\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "if not os.path.exists(output_zip):\n",
    "    gdown.download(url, output_zip, quiet=False)\n",
    "\n",
    "with zipfile.ZipFile(output_zip, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15dfc38",
   "metadata": {},
   "source": [
    "<a name=\"download-coco-people-subset-dataset\"></a>\n",
    "\n",
    "\n",
    "## ‚öôÔ∏è Creating a Custom Generator\n",
    "\n",
    "üëâ For additional information about the LuxonisML annotation format, check out the [LuxonisML Annotation Format](https://github.com/luxonis/luxonis-ml/blob/main/luxonis_ml/data/README.md#annotation-format).\n",
    "\n",
    "`LuxonisDataset` expects your generator to yield **dictionary per instance on a image** with the following structure:\n",
    "\n",
    "### Main Structure\n",
    "\n",
    "- **`file`** (`str`):  \n",
    "  Absolute or relative path to the image file.\n",
    "\n",
    "- **`annotation`** (`dict` or `None`):  \n",
    "  All labels for *one* object in the image.  \n",
    "  *(Omit if the image is un-annotated.)*\n",
    "\n",
    "- **`task_name`** (`str` or `None`):  \n",
    "  Optional label for **multi-task datasets**.  \n",
    "  Use it to group different annotation types (e.g., keypoints vs. segmentation) to train separate models for each task.\n",
    "\n",
    "---\n",
    "\n",
    "### `annotation` Schema\n",
    "\n",
    "Each annotation dictionary may include:\n",
    "\n",
    "- **`class`** (`str`):  \n",
    "  Object label (e.g., `\"person\"`).\n",
    "\n",
    "- **`boundingbox`** (`dict`):  \n",
    "  Normalized **[0‚Äì1]** rectangle describing the object, as fractions of the image‚Äôs width and height.  \n",
    "  Format:\n",
    "  ```python\n",
    "  {\n",
    "    \"x\": float,  # Top-left X coordinate\n",
    "    \"y\": float,  # Top-left Y coordinate\n",
    "    \"w\": float,  # Width\n",
    "    \"h\": float   # Height\n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **`segmentation`** (`dict`):  \n",
    "  Whole-object mask in one of three formats:\n",
    "  - **Binary mask** (pixel space):  \n",
    "    ```python\n",
    "    { \"mask\": numpy.ndarray }\n",
    "    ```\n",
    "    2D array of shape `(height, width)` with `0/1` (`uint8` or `bool`) values.\n",
    "\n",
    "  - **Polygon format** (coordinates normalized [0‚Äì1]):  \n",
    "    ```python\n",
    "    { \"height\": int, \"width\": int, \"points\": List[Tuple[float, float]] }\n",
    "    ```\n",
    "\n",
    "  - **RLE format** (Run-Length Encoding):  \n",
    "    ```python\n",
    "    { \"height\": int, \"width\": int, \"counts\": List[int] }\n",
    "    ```\n",
    "\n",
    "- **`instance_segmentation`** (`dict`):  \n",
    "  Per-instance mask; same structure as `segmentation`.\n",
    "\n",
    "- **`keypoints`** (`dict`):  \n",
    "  Ordered keypoints list:\n",
    "  ```python\n",
    "  { \"keypoints\": List[Tuple[float, float, int]] }\n",
    "  ```\n",
    "    Each keypoint tuple is `(x, y, v)` where:\n",
    "    - `x`, `y` are coordinates normalized to the **[0‚Äì1]** range (fractions of the image's width and height).\n",
    "    - `v` is the visibility:\n",
    "        - `v = 0`: not visible\n",
    "        - `v = 1`: occluded\n",
    "        - `v = 2`: visible\n",
    "\n",
    "- **`instance_id`** (`int` or `None`):  \n",
    "  Unique ID that links the bounding box, mask, and keypoints belonging to the *same* object. Not needed if all instance annotations are yielded together.\n",
    "\n",
    "---\n",
    "\n",
    "> **Tip:**  \n",
    "> When providing bounding boxes, masks, and keypoints for the **same object instance**, make sure to assign the same `instance_id` to each of them.  \n",
    "> This allows them to be correctly grouped together during training or evaluation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def COCO_people_subset_generator() -> DatasetIterator:\n",
    "    # find image paths and load COCO annotations\n",
    "    img_dir = \"../data/person_val2017_subset\"\n",
    "    annot_file = \"../data/person_keypoints_val2017.json\"\n",
    "    # get paths to images sorted by number\n",
    "    im_paths = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "    nums = np.array(\n",
    "        [int(os.path.splitext(os.path.basename(path))[0]) for path in im_paths]\n",
    "    )\n",
    "    idxs = np.argsort(nums)\n",
    "    im_paths = list(np.array(im_paths)[idxs])\n",
    "    # load annotations\n",
    "    with open(annot_file) as file:\n",
    "        data = json.load(file)\n",
    "    imgs = data[\"images\"]\n",
    "    anns = data[\"annotations\"]\n",
    "    # Create dictionaries for quick lookups\n",
    "    img_dict = {img[\"file_name\"]: img for img in imgs}\n",
    "    ann_dict = {}\n",
    "    for ann in anns:\n",
    "        img_id = ann[\"image_id\"]\n",
    "        if img_id not in ann_dict:\n",
    "            ann_dict[img_id] = []\n",
    "        ann_dict[img_id].append(ann)\n",
    "\n",
    "    # Process each image and its annotations\n",
    "    for path in tqdm(im_paths):\n",
    "        # Find annotations matching the COCO image\n",
    "        gran = os.path.basename(path)\n",
    "        img = img_dict.get(gran)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img_id = img[\"id\"]\n",
    "        img_anns = ann_dict.get(img_id, [])\n",
    "\n",
    "        # Load the image\n",
    "        im = cv2.imread(path)\n",
    "        height, width, _ = im.shape\n",
    "\n",
    "        # Process each annotation\n",
    "        for i, ann in enumerate(img_anns):\n",
    "            # Create a base record with the file and instance ID\n",
    "            record = {\n",
    "                \"file\": path,\n",
    "                \"annotation\": {\n",
    "                    \"class\": \"person\",\n",
    "                    \"instance_id\": i,\n",
    "                },\n",
    "            }\n",
    "\n",
    "            # Add bounding box to record\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            record[\"annotation\"][\"boundingbox\"] = {\n",
    "                \"x\": x / width,\n",
    "                \"y\": y / height,\n",
    "                \"w\": w / width,\n",
    "                \"h\": h / height,\n",
    "            }\n",
    "\n",
    "            # Process segmentation\n",
    "            seg = ann[\"segmentation\"]\n",
    "            if isinstance(seg, list) and seg:  # polygon format\n",
    "                poly = []\n",
    "                for s in seg:\n",
    "                    poly_arr = np.array(s).reshape(-1, 2)\n",
    "                    poly += [\n",
    "                        (poly_arr[j, 0] / width, poly_arr[j, 1] / height)\n",
    "                        for j in range(len(poly_arr))\n",
    "                    ]\n",
    "                segmentation = {\n",
    "                    \"height\": height,\n",
    "                    \"width\": width,\n",
    "                    \"points\": poly,\n",
    "                }\n",
    "                record[\"annotation\"][\"segmentation\"] = segmentation\n",
    "                record[\"annotation\"][\"instance_segmentation\"] = segmentation\n",
    "            elif isinstance(seg, dict):  # RLE format\n",
    "                segmentation = {\n",
    "                    \"height\": seg[\"size\"][0],\n",
    "                    \"width\": seg[\"size\"][1],\n",
    "                    \"counts\": seg[\"counts\"],\n",
    "                }\n",
    "                record[\"annotation\"][\"segmentation\"] = segmentation\n",
    "                record[\"annotation\"][\"instance_segmentation\"] = segmentation\n",
    "\n",
    "            # Add keypoints to record\n",
    "            if \"keypoints\" in ann:\n",
    "                kps = np.array(ann[\"keypoints\"]).reshape(-1, 3)\n",
    "                keypoints = []\n",
    "                for kp in kps:\n",
    "                    # Clip keypoints to image boundaries\n",
    "                    x = np.clip(kp[0], 0, width)\n",
    "                    y = np.clip(kp[1], 0, height)\n",
    "                    keypoints.append((x / width, y / height, int(kp[2])))\n",
    "                record[\"annotation\"][\"keypoints\"] = {\"keypoints\": keypoints}\n",
    "\n",
    "            # Yield the complete record with all annotations. Because we yield keypoints, bounding box and instance segmentations together, we don't need to provide the instance id's in the record.\n",
    "            yield record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1413fb4e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a name=\"creating-the-dataset-and-making-splits\"></a>\n",
    "\n",
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è Creating the Dataset and Making Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45569002",
   "metadata": {},
   "source": [
    "Below is a concise example that **creates a new dataset** and populates it with samples streamed from a generator.  \n",
    "Since we pass `delete_existing=True`, any previously existing dataset with the same name will be removed first. Without this flag, the new samples would simply be appended to the existing dataset.\n",
    "\n",
    "üëâ For more details about `LuxonisDataset`, check out the [LuxonisML LuxonisDataset Documentation](https://github.com/luxonis/luxonis-ml/blob/main/luxonis_ml/data/README.md#luxonisdataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"COCO_people_subset\"\n",
    "dataset = LuxonisDataset(dataset_name, delete_local=True)\n",
    "dataset.add(COCO_people_subset_generator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9454797-d804-45f1-92dc-393f76be2219",
   "metadata": {},
   "source": [
    "Below we randomly split the dataset into **train / val / test** subsets in an  \n",
    "**80 % / 10 % / 10 %** ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2094a5d-0371-48da-91f1-b9590686339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.make_splits(splits=(0.8, 0.1, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5d3d9",
   "metadata": {},
   "source": [
    "Need precise control?  \n",
    "Pass a `definitions` dictionary instead of `ratios`.  \n",
    "The method expects an `Optional[Dict[str, List[PathType]]]`, where  \n",
    "`PathType = Union[str, pathlib.Path]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f6d36-d5f1-4c68-9f70-80d26d45690e",
   "metadata": {},
   "source": [
    "<a name=\"inspecting-the-dataset-via-loader\"></a>\n",
    "\n",
    "## üìä Inspecting the Dataset via Loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddaf020",
   "metadata": {},
   "source": [
    "You can inspect a dataset directly from the command line:\n",
    "\n",
    "```bash\n",
    "luxonis_ml data inspect <dataset_name>\n",
    "```\n",
    "\n",
    "Other useful commands:\n",
    "\n",
    "- `luxonis_ml data health` ‚Äî run a health-check and spot common annotation issues  \n",
    "- `luxonis_ml data info`   ‚Äî print summary statistics and metadata\n",
    "\n",
    "\n",
    "üëâ For a full list of CLI commands, check out the [LuxonisML CLI Documentation](https://github.com/luxonis/luxonis-ml/blob/main/luxonis_ml/data/datasets/README.md#luxonisml-cli).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c65858",
   "metadata": {},
   "source": [
    "Using the Python API instead of the CLI\n",
    "In the example below we skip the CLI and traverse the train split with `LuxonisLoader`, then visualise bounding boxes, masks and key-points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda91cd6-9fe5-43ee-ab88-3dfc57ff89ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = LuxonisLoader(dataset, view=\"train\")\n",
    "for image, ann in loader:\n",
    "    cls = ann[\"/classification\"]\n",
    "    box = ann[\"/boundingbox\"]\n",
    "    seg = ann[\"/segmentation\"]\n",
    "    kps = ann[\"/keypoints\"]\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    for b in box:\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(b[1] * w), int(b[2] * h)),\n",
    "            (int(b[1] * w + b[3] * w), int(b[2] * h + b[4] * h)),\n",
    "            (255, 0, 0),\n",
    "            2,\n",
    "        )\n",
    "    mask_viz = np.zeros((h, w, 3)).astype(np.uint8)\n",
    "    for mask in seg:\n",
    "        mask_viz[mask == 1, 2] = 255\n",
    "    image = cv2.addWeighted(image, 0.5, mask_viz, 0.5, 0)\n",
    "\n",
    "    for kp in kps:\n",
    "        kp = kp.reshape(-1, 3)\n",
    "        for k in kp:\n",
    "            cv2.circle(\n",
    "                image, (int(k[0] * w), int(k[1] * h)), 2, (0, 255, 0), 2\n",
    "            )\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
